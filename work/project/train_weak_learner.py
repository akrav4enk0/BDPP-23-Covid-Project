"""
warning: autogenerated by writefile2. do not edit!
"""

"""
train a weak learner using given right/left window lengths and lag.
for learning a (rank-deficient) linear function of the right window use the linear operator.
the weak learner can be a linear model or a random forest.
trigger this node with verbose settings to see some diagnostic and theory output in LaTeX.
"""

import nodes
import require

@nodes.generic_node
def train_weak_learner( 

        subset = slice( None ), 
        length_l = 1, 
        lag = 0, 
        length_r = 1, 
        linear_operator = None, 
        weight = 1, 
        type = "lm",
        learner_kwargs = { }
    ):

    import numpy as np
    import pandas as pd
    from IPython.display import display, HTML, Markdown as md
    from tqdm import tqdm
    import sys
    from types import SimpleNamespace as ns
    import os
        
    weak_learner = require.single( "weak_learner" )
    get_number_of_window_samples = require.single( "get_number_of_window_samples" )
    display_dict = require.untracked.single( "display_dict" )
    verbose = require.untracked.single( "verbose" )
    n_outcomes = len( require.single( "owid_outcomes" ))

    if linear_operator is None:

        #average
        linear_operator = np.ones(( 1, length_r )) / length_r
    
    patch_args = dict( 

        subset = subset,
        length_l = length_l, 
        lag = lag, 
        length_r = length_r, 
        linear_operator = linear_operator 
    )
    
    def main( 
        
            patches_node: nodes.find( "window_patches" ).given( ** patch_args ),
            training_data_node: nodes.find( "training_data" ).given( )
        ):

        self = weak_learner( )
        X, Y = patches_node.result
        
        dataframes = training_data_node.result[ subset ] #currently simple slice indexing

        #number of time series
        d = dataframes[ 0 ].shape[ 1 ]
        get_n_samples = lambda df: get_number_of_window_samples( df, length_l, lag, length_r )
        n_samples_total = sum([ get_n_samples( df ) for df in dataframes ])
    
        if verbose( ):
    
            display( HTML( f"<h1>Weak Learner Training</h1>" ))
            display( HTML( f"<h3>Parameters</h3>" ))
            
            display_dict({
                
                "number of dataframes": len( dataframes ),
                ** { f"samples from dataframe { i }": get_n_samples( dataframes[ i ]) for i in range( len( dataframes ))},
                "total number of samples": n_samples_total,
                "number of time series": d,
                "number of outcome series": n_outcomes,
                "length of left/predictor window": length_l, 
                "lag/spacing between windows": lag, 
                "length of right/response window": length_r,
                "shape of linear operator M": linear_operator.shape,
                "least squares weight": weight
            })
    
        def get_weak_learner( X, Y ):

            if type == "lm":
            
                from sklearn.linear_model import LinearRegression as lm
                return lm( ** learner_kwargs ).fit( X, Y )

            if type == "forest":

                from sklearn.ensemble import RandomForestRegressor as rf

                args = ns( ** learner_kwargs )
                
                if not hasattr( args, "n_jobs" ): 
                    
                    args.n_jobs = 1
                
                if args.n_jobs < 0:

                    args.n_jobs = os.cpu_count( ) + 1 + args.n_jobs

                if not hasattr( args, "n_estimators" ):

                    args.n_estimators = 100

                n_estimators = args.n_estimators
                n_jobs = args.n_jobs
                print( args.__dict__ )
                del args.n_estimators

                n_steps = int( n_estimators / n_jobs )
                m = rf( warm_start = True, ** args.__dict__ )

                for i in tqdm( range( n_steps ), file = sys.stdout, desc = "training estimators" ):

                    m.set_params( n_estimators = n_jobs * ( i + 1 ))
                    m.fit( X, Y )
                
                return m        
        
        print( "training..." )
        model = get_weak_learner( X, Y )
        print( "done" )
    
        self.__dict__.update( 

            model = model,
            length_l = length_l, 
            lag = lag,
            length_r = length_r,
            linear_operator = linear_operator,
            weight = weight,
            d = d,
            n_outcomes = n_outcomes
        )

        return self

    return main

node = train_weak_learner
